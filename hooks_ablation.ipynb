{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/gyk86ZaU6a6vZqnJl+CT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdelhadidjafer02-beep/GPT-2/blob/main/hooks_ablation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Setup & The Proxy Task\n",
        "\n",
        "We use a repeatable prompt where the model must look back at previous context to get the answer right.\n"
      ],
      "metadata": {
        "id": "tpf0vflgjb8K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPO5kdMxbZrY"
      },
      "outputs": [],
      "source": [
        "# Setup (Standard)\n",
        "!pip install transformer_lens\n",
        "import torch\n",
        "from transformer_lens import HookedTransformer\n",
        "import transformer_lens.utils as utils\n",
        "\n",
        "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
        "\n",
        "# The Proxy Task: A \"Repeated Random\" sequence\n",
        "# If the model sees \" A B C ... A\", it should predict \" B\".\n",
        "# This isolates the \"copying\" mechanism from factual knowledge.\n",
        "text = \"The quick brown fox jumps over the lazy dog. The quick brown fox\"\n",
        "target = \" jumps\"\n",
        "\n",
        "# Measure Baseline (The \"Control\" Group)\n",
        "logits = model(text)\n",
        "prob = torch.softmax(logits[0, -1], dim=0)[model.to_single_token(target)].item()\n",
        "print(f\"Baseline Probability of '{target}': {prob:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Formulating a Hypothesis (The \"Look\")\n",
        "\n",
        "\n",
        "Before intervening, we look. We suspect Attention Heads are responsible. We hypothesize that specific heads in the middle layers (Layer 5-7 in GPT-2 Small) act as \"Induction Heads\"â€”they look at the previous instance of the current token to find what came next.\n",
        "\n",
        "Hypothesis: \"If we remove the output of Layer 5, Head 5 (L5H5), the model will forget how to copy.\""
      ],
      "metadata": {
        "id": "3wtHKpoajoet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: The Necessity Test (Ablation Hook)\n",
        "\n",
        "Here, we do targeted ablation (surgical removal).\n",
        "\n",
        "Logic: If L5H5 is necessary, zeroing it out should drop the probability of \" jumps\" significantly."
      ],
      "metadata": {
        "id": "Agy0eD7Fj_jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mean_ablation_hook(head_to_ablate):\n",
        "    def mean_ablation_hook(value, hook):\n",
        "        # value shape: [batch, pos, head_index, d_head]\n",
        "        # To truly ablate a specific head by replacing with a *mean activation* baseline,\n",
        "        # we replace its output with the mean across ALL heads in that layer\n",
        "        # for each batch and position. This removes the unique information of the ablated head.\n",
        "        mean_activation_across_heads = value.mean(dim=2, keepdim=True)\n",
        "        value[:, :, head_to_ablate, :] = mean_activation_across_heads[:, :, 0, :]\n",
        "        return value\n",
        "    return mean_ablation_hook\n",
        "\n",
        "# Store results\n",
        "ablation_results = {}\n",
        "\n",
        "# Loop through all layers and all heads\n",
        "num_layers = model.cfg.n_layers # GPT2-small has 12 layers\n",
        "num_heads = model.cfg.n_heads   # GPT2-small has 12 heads per layer\n",
        "\n",
        "print(f\"Baseline Probability of '{target}': {prob:.2%}\\n\")\n",
        "\n",
        "for layer_idx in range(num_layers):\n",
        "    for head_idx in range(num_heads):\n",
        "        hook_name = f\"blocks.{layer_idx}.attn.hook_z\"\n",
        "        specific_ablation_hook = make_mean_ablation_hook(head_idx)\n",
        "\n",
        "        ablated_logits = model.run_with_hooks(\n",
        "            text,\n",
        "            fwd_hooks=[(hook_name, specific_ablation_hook)]\n",
        "        )\n",
        "\n",
        "        ablated_prob = torch.softmax(ablated_logits[0, -1], dim=0)[model.to_single_token(target)].item()\n",
        "        ablation_results[f\"L{layer_idx}H{head_idx}\"] = ablated_prob\n",
        "\n",
        "        print(f\"Prob after ablating L{layer_idx}H{head_idx}: {ablated_prob:.2%} (Change: {(ablated_prob - prob):.2%})\")\n",
        "\n",
        "# You can also analyze ablation_results dictionary further if needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzQcOZ86kdYV",
        "outputId": "9dab51a5-b605-4b0c-ff3a-686299821f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Probability of ' jumps': 47.07%\n",
            "\n",
            "Prob after ablating L0H0: 42.68% (Change: -4.39%)\n",
            "Prob after ablating L0H1: 52.80% (Change: 5.73%)\n",
            "Prob after ablating L0H2: 39.03% (Change: -8.04%)\n",
            "Prob after ablating L0H3: 57.89% (Change: 10.82%)\n",
            "Prob after ablating L0H4: 41.43% (Change: -5.64%)\n",
            "Prob after ablating L0H5: 48.64% (Change: 1.57%)\n",
            "Prob after ablating L0H6: 59.33% (Change: 12.26%)\n",
            "Prob after ablating L0H7: 17.29% (Change: -29.78%)\n",
            "Prob after ablating L0H8: 4.86% (Change: -42.21%)\n",
            "Prob after ablating L0H9: 52.86% (Change: 5.79%)\n",
            "Prob after ablating L0H10: 44.71% (Change: -2.36%)\n",
            "Prob after ablating L0H11: 33.07% (Change: -14.00%)\n",
            "Prob after ablating L1H0: 41.96% (Change: -5.11%)\n",
            "Prob after ablating L1H1: 36.00% (Change: -11.07%)\n",
            "Prob after ablating L1H2: 46.90% (Change: -0.17%)\n",
            "Prob after ablating L1H3: 36.93% (Change: -10.14%)\n",
            "Prob after ablating L1H4: 42.53% (Change: -4.54%)\n",
            "Prob after ablating L1H5: 38.68% (Change: -8.39%)\n",
            "Prob after ablating L1H6: 51.59% (Change: 4.52%)\n",
            "Prob after ablating L1H7: 39.16% (Change: -7.91%)\n",
            "Prob after ablating L1H8: 38.41% (Change: -8.66%)\n",
            "Prob after ablating L1H9: 51.27% (Change: 4.19%)\n",
            "Prob after ablating L1H10: 51.87% (Change: 4.80%)\n",
            "Prob after ablating L1H11: 64.39% (Change: 17.32%)\n",
            "Prob after ablating L2H0: 65.42% (Change: 18.35%)\n",
            "Prob after ablating L2H1: 41.17% (Change: -5.90%)\n",
            "Prob after ablating L2H2: 49.83% (Change: 2.76%)\n",
            "Prob after ablating L2H3: 67.18% (Change: 20.11%)\n",
            "Prob after ablating L2H4: 43.32% (Change: -3.75%)\n",
            "Prob after ablating L2H5: 51.44% (Change: 4.37%)\n",
            "Prob after ablating L2H6: 48.62% (Change: 1.55%)\n",
            "Prob after ablating L2H7: 45.02% (Change: -2.05%)\n",
            "Prob after ablating L2H8: 78.08% (Change: 31.01%)\n",
            "Prob after ablating L2H9: 45.43% (Change: -1.64%)\n",
            "Prob after ablating L2H10: 53.39% (Change: 6.32%)\n",
            "Prob after ablating L2H11: 34.29% (Change: -12.78%)\n",
            "Prob after ablating L3H0: 32.79% (Change: -14.28%)\n",
            "Prob after ablating L3H1: 50.53% (Change: 3.46%)\n",
            "Prob after ablating L3H2: 50.79% (Change: 3.72%)\n",
            "Prob after ablating L3H3: 53.10% (Change: 6.03%)\n",
            "Prob after ablating L3H4: 52.76% (Change: 5.69%)\n",
            "Prob after ablating L3H5: 40.67% (Change: -6.40%)\n",
            "Prob after ablating L3H6: 50.22% (Change: 3.15%)\n",
            "Prob after ablating L3H7: 37.07% (Change: -10.00%)\n",
            "Prob after ablating L3H8: 52.82% (Change: 5.75%)\n",
            "Prob after ablating L3H9: 45.21% (Change: -1.86%)\n",
            "Prob after ablating L3H10: 43.70% (Change: -3.37%)\n",
            "Prob after ablating L3H11: 55.98% (Change: 8.91%)\n",
            "Prob after ablating L4H0: 46.25% (Change: -0.82%)\n",
            "Prob after ablating L4H1: 48.20% (Change: 1.13%)\n",
            "Prob after ablating L4H2: 51.59% (Change: 4.52%)\n",
            "Prob after ablating L4H3: 51.79% (Change: 4.72%)\n",
            "Prob after ablating L4H4: 45.21% (Change: -1.86%)\n",
            "Prob after ablating L4H5: 47.51% (Change: 0.44%)\n",
            "Prob after ablating L4H6: 36.42% (Change: -10.65%)\n",
            "Prob after ablating L4H7: 56.38% (Change: 9.31%)\n",
            "Prob after ablating L4H8: 47.13% (Change: 0.06%)\n",
            "Prob after ablating L4H9: 45.16% (Change: -1.91%)\n",
            "Prob after ablating L4H10: 46.36% (Change: -0.71%)\n",
            "Prob after ablating L4H11: 36.31% (Change: -10.76%)\n",
            "Prob after ablating L5H0: 43.22% (Change: -3.85%)\n",
            "Prob after ablating L5H1: 54.56% (Change: 7.49%)\n",
            "Prob after ablating L5H2: 47.02% (Change: -0.05%)\n",
            "Prob after ablating L5H3: 42.10% (Change: -4.97%)\n",
            "Prob after ablating L5H4: 49.94% (Change: 2.87%)\n",
            "Prob after ablating L5H5: 59.44% (Change: 12.37%)\n",
            "Prob after ablating L5H6: 52.97% (Change: 5.90%)\n",
            "Prob after ablating L5H7: 51.38% (Change: 4.31%)\n",
            "Prob after ablating L5H8: 46.50% (Change: -0.57%)\n",
            "Prob after ablating L5H9: 62.94% (Change: 15.87%)\n",
            "Prob after ablating L5H10: 48.66% (Change: 1.59%)\n",
            "Prob after ablating L5H11: 31.38% (Change: -15.69%)\n",
            "Prob after ablating L6H0: 51.70% (Change: 4.63%)\n",
            "Prob after ablating L6H1: 49.12% (Change: 2.05%)\n",
            "Prob after ablating L6H2: 40.41% (Change: -6.66%)\n",
            "Prob after ablating L6H3: 35.54% (Change: -11.53%)\n",
            "Prob after ablating L6H4: 56.73% (Change: 9.66%)\n",
            "Prob after ablating L6H5: 34.69% (Change: -12.38%)\n",
            "Prob after ablating L6H6: 55.19% (Change: 8.12%)\n",
            "Prob after ablating L6H7: 40.18% (Change: -6.89%)\n",
            "Prob after ablating L6H8: 43.59% (Change: -3.48%)\n",
            "Prob after ablating L6H9: 49.01% (Change: 1.94%)\n",
            "Prob after ablating L6H10: 34.27% (Change: -12.80%)\n",
            "Prob after ablating L6H11: 45.73% (Change: -1.35%)\n",
            "Prob after ablating L7H0: 47.94% (Change: 0.87%)\n",
            "Prob after ablating L7H1: 50.38% (Change: 3.31%)\n",
            "Prob after ablating L7H2: 43.88% (Change: -3.19%)\n",
            "Prob after ablating L7H3: 41.88% (Change: -5.19%)\n",
            "Prob after ablating L7H4: 44.04% (Change: -3.03%)\n",
            "Prob after ablating L7H5: 54.14% (Change: 7.07%)\n",
            "Prob after ablating L7H6: 21.72% (Change: -25.35%)\n",
            "Prob after ablating L7H7: 53.14% (Change: 6.07%)\n",
            "Prob after ablating L7H8: 41.91% (Change: -5.16%)\n",
            "Prob after ablating L7H9: 48.77% (Change: 1.70%)\n",
            "Prob after ablating L7H10: 66.94% (Change: 19.87%)\n",
            "Prob after ablating L7H11: 48.23% (Change: 1.16%)\n",
            "Prob after ablating L8H0: 38.72% (Change: -8.35%)\n",
            "Prob after ablating L8H1: 41.95% (Change: -5.12%)\n",
            "Prob after ablating L8H2: 49.86% (Change: 2.79%)\n",
            "Prob after ablating L8H3: 47.04% (Change: -0.03%)\n",
            "Prob after ablating L8H4: 49.44% (Change: 2.37%)\n",
            "Prob after ablating L8H5: 39.16% (Change: -7.91%)\n",
            "Prob after ablating L8H6: 59.94% (Change: 12.87%)\n",
            "Prob after ablating L8H7: 48.38% (Change: 1.31%)\n",
            "Prob after ablating L8H8: 51.96% (Change: 4.89%)\n",
            "Prob after ablating L8H9: 43.56% (Change: -3.51%)\n",
            "Prob after ablating L8H10: 47.31% (Change: 0.24%)\n",
            "Prob after ablating L8H11: 47.80% (Change: 0.73%)\n",
            "Prob after ablating L9H0: 46.35% (Change: -0.72%)\n",
            "Prob after ablating L9H1: 48.84% (Change: 1.77%)\n",
            "Prob after ablating L9H2: 48.36% (Change: 1.29%)\n",
            "Prob after ablating L9H3: 39.98% (Change: -7.09%)\n",
            "Prob after ablating L9H4: 43.13% (Change: -3.94%)\n",
            "Prob after ablating L9H5: 50.38% (Change: 3.31%)\n",
            "Prob after ablating L9H6: 47.33% (Change: 0.26%)\n",
            "Prob after ablating L9H7: 50.41% (Change: 3.34%)\n",
            "Prob after ablating L9H8: 46.17% (Change: -0.90%)\n",
            "Prob after ablating L9H9: 46.00% (Change: -1.07%)\n",
            "Prob after ablating L9H10: 45.96% (Change: -1.11%)\n",
            "Prob after ablating L9H11: 46.60% (Change: -0.47%)\n",
            "Prob after ablating L10H0: 37.74% (Change: -9.33%)\n",
            "Prob after ablating L10H1: 40.23% (Change: -6.84%)\n",
            "Prob after ablating L10H2: 44.39% (Change: -2.68%)\n",
            "Prob after ablating L10H3: 47.34% (Change: 0.27%)\n",
            "Prob after ablating L10H4: 45.76% (Change: -1.31%)\n",
            "Prob after ablating L10H5: 53.37% (Change: 6.30%)\n",
            "Prob after ablating L10H6: 46.21% (Change: -0.86%)\n",
            "Prob after ablating L10H7: 62.81% (Change: 15.74%)\n",
            "Prob after ablating L10H8: 46.03% (Change: -1.04%)\n",
            "Prob after ablating L10H9: 48.68% (Change: 1.61%)\n",
            "Prob after ablating L10H10: 41.59% (Change: -5.48%)\n",
            "Prob after ablating L10H11: 46.21% (Change: -0.86%)\n",
            "Prob after ablating L11H0: 16.24% (Change: -30.83%)\n",
            "Prob after ablating L11H1: 46.42% (Change: -0.65%)\n",
            "Prob after ablating L11H2: 43.18% (Change: -3.89%)\n",
            "Prob after ablating L11H3: 46.76% (Change: -0.31%)\n",
            "Prob after ablating L11H4: 46.52% (Change: -0.55%)\n",
            "Prob after ablating L11H5: 44.15% (Change: -2.92%)\n",
            "Prob after ablating L11H6: 45.66% (Change: -1.41%)\n",
            "Prob after ablating L11H7: 46.94% (Change: -0.13%)\n",
            "Prob after ablating L11H8: 48.02% (Change: 0.95%)\n",
            "Prob after ablating L11H9: 44.16% (Change: -2.91%)\n",
            "Prob after ablating L11H10: 54.55% (Change: 7.48%)\n",
            "Prob after ablating L11H11: 45.01% (Change: -2.06%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc976ed6"
      },
      "source": [
        "## Ablation Experiment Results Explanation\n",
        "\n",
        "The ablation experiment has completed, and we can now see the impact of each attention head on the prediction of ' jumps'.\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "*   **Significant Drops:** Some heads, like `L0H8` (a -42.21% change, dropping probability to 4.86%), `L0H7` (a -29.78% change, dropping probability to 17.29%), and `L11H0` (a -30.83% change, dropping probability to 16.24%), caused very large decreases in the probability of predicting ' jumps'. This suggests these heads are crucial for the model's ability to perform the copying task.\n",
        "\n",
        "*   **Significant Increases:** Conversely, some heads, such as `L2H8` (a +31.01% change, increasing probability to 78.08%), `L2H3` (a +20.11% change, increasing probability to 67.18%), and `L7H10` (a +19.87% change, increasing probability to 66.94%), led to a substantial *increase* in the probability. This indicates these heads might normally be suppressing the correct prediction or are involved in other less direct mechanisms.\n",
        "\n",
        "*   **Mixed Impact:** Many other heads caused smaller positive or negative changes, suggesting they contribute to the task to varying degrees, or their roles are less direct.\n",
        "\n",
        "This analysis helps us pinpoint specific attention heads that are critical for the model's behavior in this proxy task. The original hypothesis focused on middle layers, and while some of those are important, we also see critical heads in earlier (L0) and later (L11) layers."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mihcnJ2oDkt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}