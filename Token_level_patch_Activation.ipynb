{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO++4rGNmwWUybZiZX47ZEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdelhadidjafer02-beep/GPT-2/blob/main/Token_level_patch_Activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Token-Level Activation Patching"
      ],
      "metadata": {
        "id": "agQLTUq9rPbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformer_lens\n",
        "\n",
        "import torch\n",
        "from transformer_lens import HookedTransformer\n",
        "import pandas as pd\n",
        "\n",
        "# Load a small, manageable model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8GWytJoPxJl",
        "outputId": "659d9042-3eeb-48c7-90bd-6518ba46a02a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_prompt = \"LeBron James plays the sport of\"\n",
        "corrupted_prompt = \"LeBron James plays the profession of\"\n",
        "# We change one word to see where the 'sport' concept is injected.\n",
        "\n",
        "clean_tokens = model.to_tokens(clean_prompt)\n",
        "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
        "\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
        "\n",
        "# The token for ' Basketball'\n",
        "answer_token = model.to_single_token(\" Basketball\")"
      ],
      "metadata": {
        "id": "W2qX_6q8P0_v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b9343c3",
        "outputId": "1d5338ac-1cd7-4e09-cd32-dd88cce9ecfd"
      },
      "source": [
        "clean_logit_basketball = clean_logits[0, -1, answer_token].item()\n",
        "corrupted_logit_basketball = corrupted_logits[0, -1, answer_token].item()\n",
        "\n",
        "print(f\"Clean logit for ' Basketball': {clean_logit_basketball:.4f}\")\n",
        "print(f\"Corrupted logit for ' Basketball': {corrupted_logit_basketball:.4f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean logit for ' Basketball': 11.6198\n",
            "Corrupted logit for ' Basketball': 9.7076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a05b140f",
        "outputId": "2100debb-0d15-455a-f63d-aff3820265c3"
      },
      "source": [
        "# Identify the token index for 'sport'/'profession' which is the last token in our prompts\n",
        "token_to_patch_idx = clean_tokens.shape[1] - 1\n",
        "\n",
        "def patch_residual_stream_token_pos(target_layer):\n",
        "    def patch_hook(clean_residual, hook):\n",
        "        # Patch only at the relevant token position (the last one)\n",
        "        clean_residual[:, token_to_patch_idx, :] = corrupted_cache[hook.name][:, token_to_patch_idx, :]\n",
        "        return clean_residual\n",
        "\n",
        "    patched_logits = model.run_with_hooks(\n",
        "        clean_tokens,\n",
        "        fwd_hooks=[(f\"blocks.{target_layer}.hook_resid_post\", patch_hook)]\n",
        "    )\n",
        "\n",
        "    return patched_logits[0, -1, answer_token].item()\n",
        "\n",
        "# Test with the modified function for a specific layer\n",
        "print(f\"Logit for Basketball with token-level patch at Layer 5: {patch_residual_stream_token_pos(5):.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit for Basketball with token-level patch at Layer 5: 10.6327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6feef4f7",
        "outputId": "c3ddd713-454b-4b92-f939-fe959c3d0d42"
      },
      "source": [
        "results_token_patch = []\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    logit_score = patch_residual_stream_token_pos(layer)\n",
        "    results_token_patch.append({\"layer\": layer, \"logit\": logit_score})\n",
        "\n",
        "df_token_patch = pd.DataFrame(results_token_patch)\n",
        "print(df_token_patch)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    layer      logit\n",
            "0       0  11.643492\n",
            "1       1  11.771039\n",
            "2       2  11.786404\n",
            "3       3  11.112857\n",
            "4       4  11.311896\n",
            "5       5  10.632686\n",
            "6       6  10.531447\n",
            "7       7  10.219274\n",
            "8       8   9.889150\n",
            "9       9   9.869099\n",
            "10     10   9.868301\n",
            "11     11   9.707586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736ee3cb"
      },
      "source": [
        "### Explanation of Token-Level Patching Results\n",
        "\n",
        "Looking at `df_token_patch`:\n",
        "\n",
        "*   **Initial Layers (0-2):** The logit for ' Basketball' is very close to the `clean_logit_basketball` (around 11.6 to 11.7). This indicates that when we patch at these early layers, the impact on the final prediction is minimal because the key distinguishing information (between 'sport' and 'profession') hasn't been fully processed yet.\n",
        "*   **Mid-Layers (3-7):** We start to see a more noticeable drop in the logit score (e.g., from 11.11 at Layer 3 down to 10.22 at Layer 7). This suggests that the model is beginning to integrate the 'corrupted' information ('profession' instead of 'sport') at the final token position, leading to a decreased prediction for ' Basketball'.\n",
        "*   **Later Layers (8-11):** The logit continues to decrease, eventually reaching `9.707586` at layer 11. This is almost identical to the `corrupted_logit_basketball` (9.7076).\n",
        "\n",
        "**What does this tell us?**\n",
        "\n",
        "Unlike the previous full-residual patching where the logit immediately dropped to the corrupted value, this token-level patching shows a **gradual decrease**. This indicates that the information distinguishing 'sport' from 'profession' (and thus influencing the prediction for ' Basketball') is not fully processed in a single layer. Instead, it's progressively built up and refined across the transformer layers, with significant changes becoming apparent from the mid-layers onwards. By layer 11, the model has fully incorporated the 'corrupted' information at that specific token position."
      ]
    }
  ]
}